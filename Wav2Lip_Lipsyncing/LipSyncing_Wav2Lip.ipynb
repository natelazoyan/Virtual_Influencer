{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "802K_fw2kLbV"
      },
      "source": [
        "LipSyncing with Wav2Lip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "EY2ZpSRrsE4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import requests\n",
        "\n",
        "OPENAI_API_KEY = \"  \" #@param\n",
        "\n",
        "def translate_to_armenian(sentence):\n",
        "    # Construct the prompt for translation\n",
        "    prompt = f\"Translate the following text into Armenian: '{sentence}'\"\n",
        "\n",
        "    # Set up the API request headers\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {OPENAI_API_KEY}',\n",
        "    }\n",
        "\n",
        "    # Construct the request data\n",
        "    data = {\n",
        "        'model': 'gpt-3.5-turbo-instruct',  # Specify the model for translation\n",
        "        'prompt': prompt,\n",
        "        'temperature': 0,  # Set temperature to 0 for deterministic output\n",
        "        'max_tokens': 300  # Limit the number of tokens generated\n",
        "    }\n",
        "\n",
        "    # Send the request to the OpenAI API\n",
        "    response = requests.post('https://api.openai.com/v1/completions', json=data, headers=headers)\n",
        "\n",
        "    # Process the response\n",
        "    if 'choices' in response.json() and response.json()['choices']:\n",
        "        translated_text = response.json()['choices'][0]['text'].strip()\n",
        "        return translated_text\n",
        "    else:\n",
        "        print(\"Error: Unable to translate text to Armenian.\")\n",
        "        print(\"Full response:\", response.json())\n",
        "        return None\n",
        "\n",
        "def create_audio_file(text, output_file_path):\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "    response = client.audio.speech.create(\n",
        "        model=\"tts-1-hd\",\n",
        "        voice=\"alloy\",\n",
        "        input=text\n",
        "    )\n",
        "\n",
        "    with open(output_file_path, 'wb') as audio_file:\n",
        "        for data in response.iter_bytes():\n",
        "            audio_file.write(data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentence_to_translate = \"Hi, how are you? I am Sahmi.\" #@param\n",
        "    translated_sentence = translate_to_armenian(sentence_to_translate)\n",
        "\n",
        "    if translated_sentence:\n",
        "        print(f\"Translated sentence: {translated_sentence}\")\n",
        "\n",
        "        # Specify the filename for the audio file\n",
        "        output_audio_file_path = \"/content/Translation.mp3\" #@param\n",
        "\n",
        "        # Create the audio file for the translated sentence\n",
        "        create_audio_file(translated_sentence, output_audio_file_path)\n",
        "        print(f\"Audio file '{output_audio_file_path}' created successfully.\")\n",
        "    else:\n",
        "        print(\"Error: Translation failed.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IqmvYJ5boBJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LipSyncing!\n",
        "#@markdown 1. Choose audio (you can also enter a YouTube or similar URL, or a manually uploaded file name):\n",
        "audio = '/content/Translation.mp3' # @param\n",
        "#@markdown 2. Optionally untick \"smooth_face_detection\" to disable temporal smoothing of face coordinates:\n",
        "smooth_face_detection = False #@param {type: \"boolean\"}\n",
        "#@markdown 3. Optionally tick \"override_face_detection\" to manually asign face coordinates:\n",
        "override_face_detection = False #@param {type: \"boolean\"}\n",
        "left = 0# @param {type: \"integer\"}\n",
        "top = 0# @param {type: \"integer\"}\n",
        "width = 1080 #@param {type: \"integer\"}\n",
        "height = 1920 #@param {type: \"integer\"}\n",
        "#@markdown 4. Optionally tick \"switch_speakers\" to switch between visual media files with the change of speakers:\n",
        "switch_speakers = True #@param {type: \"boolean\"}\n",
        "#@markdown 5. Choose model for speaker diarization:\n",
        "model = 'pyannote-audio AMI' #@param ['pyAudioAnalysis', 'pyannote-audio DIHARD','pyannote-audio AMI']\n",
        "#@markdown 6. Optionally tick \"reuse_files\" to reuse previously uploaded files:\n",
        "reuse_files = False #@param {type: \"boolean\"}\n",
        "#@markdo×¦wn 7. Press the play (triangle) button on the left.\n",
        "#@markdown 8. Press \"Browse\" or \"Choose files\" below, and upload image(s) or video(s) (if not reusing files).\n",
        "#@markdown 9. If the resulting videos are too large, the Colab might disconnect, but you may still manually download the .mp4 from the folder on the left (click \"Refresh\" if missing).\n",
        "\n",
        "from google.colab import files\n",
        "try:\n",
        "  inputs\n",
        "except NameError:\n",
        "  reuse_files = False\n",
        "\n",
        "if not reuse_files:\n",
        "  %cd /content/sample_data\n",
        "  !rm -rf *\n",
        "  inputs = files.upload()\n",
        "\n",
        "if inputs:\n",
        "  %cd /content\n",
        "  !git clone --depth 1 https://github.com/eyaler/Wav2Lip\n",
        "  import os\n",
        "  !pip install librosa==0.9.2\n",
        "  !pip install -U gdown\n",
        "  if not os.path.exists('/content/Wav2Lip/checkpoints/wav2lip_gan.pth'):\n",
        "    !gdown https://drive.google.com/uc?id=1dwHujX7RVNCvdR1RR93z0FS2T2yzqup9 -O /content/Wav2Lip/checkpoints/wav2lip_gan.pth\n",
        "  !wget --no-check-certificate -nc https://eyalgruss.com/fomm/wav2lip_gan.pth -O /content/Wav2Lip/checkpoints/wav2lip_gan.pth\n",
        "  #!wget --no-check-certificate -nc https://eyalgruss.com/fomm/wav2lip.pth -O /content/Wav2Lip/checkpoints/wav2lip.pth\n",
        "  !wget --no-check-certificate -nc https://eyalgruss.com/fomm/s3fd-619a316812.pth -O /content/Wav2Lip/face_detection/detection/sfd/s3fd.pth\n",
        "  !pip install git+https://github.com/ytdl-org/youtube-dl\n",
        "  grab = False\n",
        "  manual = False\n",
        "\n",
        "\n",
        "  %cd /content/Wav2Lip\n",
        "  outputs = []\n",
        "  for im in inputs:\n",
        "    !rm -rf /content/Wav2Lip/temp/*\n",
        "    infile = '/content/sample_data/'+im\n",
        "    ext = infile.rsplit('.',1)[1]\n",
        "    if ext != ext.lower() or \"'\" in infile or ' ' in infile:\n",
        "      ext = ext.lower()\n",
        "      lower = infile.rsplit('.',1)[0].replace(\"'\",'').replace(' ','_')+'.'+ext\n",
        "      !rm -rf \"$lower\"\n",
        "      os.rename(infile, lower)\n",
        "      infile = lower\n",
        "    outfile = '/content/'+im.rsplit('.',1)[0].replace(\"'\",'').replace(' ','_')+'_out.mp4'\n",
        "    !rm -rf \"$outfile\"\n",
        "    if grab:\n",
        "      audio = infile\n",
        "    elif \"'\" in audio:\n",
        "      fix = audio.replace(\"'\",'').replace(' ','_')\n",
        "      !rm -rf \"$fix\"\n",
        "      os.rename(audio, fix)\n",
        "      audio = fix\n",
        "    if not override_face_detection:\n",
        "      nosmooth = '' if smooth_face_detection else '--nosmooth'\n",
        "      !python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"$infile\" --audio \\\"\\\"$audio\\\"\\\" --pads 0 20 0 0 $nosmooth --outfile \\\"\\\"$outfile\\\"\\\"\n",
        "    if override_face_detection or os.path.exists('/content/Wav2Lip/temp/faulty_frame.jpg'):\n",
        "      import cv2\n",
        "      if override_face_detection:\n",
        "        print('\\nOverriding face detection')\n",
        "      else:\n",
        "        print('\\nFace not detected - will use whole frame')\n",
        "      if ext in ['jpg', 'png', 'jpeg']:\n",
        "        frame = cv2.imread(infile)\n",
        "      else:\n",
        "        video_stream = cv2.VideoCapture(infile)\n",
        "        still_reading, frame = video_stream.read()\n",
        "      y2,x2 = frame.shape[:2]\n",
        "      if override_face_detection:\n",
        "        x1 = left\n",
        "        y1 = top\n",
        "        x2 = min(left+width, x2)\n",
        "        y2 = min(top+height, y2)\n",
        "      else:\n",
        "        x1 = y1 = 0\n",
        "        if x2>y2:\n",
        "          x1 = (x2-y2)//2\n",
        "          x2 = x1+y2\n",
        "      !python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face \"$infile\" --audio \\\"\\\"$audio\\\"\\\" --box $y1 $y2 $x1 $x2 --pads 0 20 0 0 --outfile \\\"\\\"$outfile\\\"\\\"\n",
        "    outputs.append(outfile)\n",
        "\n",
        "  wav = None\n",
        "  if switch_speakers and len(outputs)>1 and not grab:\n",
        "    wav = audio.rsplit('.',1)[0]+'.wav'\n",
        "    !ffmpeg -i \"$audio\" \"$wav\" -y\n",
        "    min_dt = 0.5\n",
        "    if model.startswith('pyannote-audio'):\n",
        "      !pip install pyannote.audio==1.1.1\n",
        "      import torch\n",
        "      import pyannote.core #https://github.com/pyannote/pyannote-audio/issues/561\n",
        "      from pyannote.audio.features.utils import get_audio_duration\n",
        "      if model.endswith('AMI'):\n",
        "        pipeline = torch.hub.load('pyannote/pyannote-audio', 'dia_ami')\n",
        "      else:\n",
        "        pipeline = torch.hub.load('pyannote/pyannote-audio', 'dia')\n",
        "      cls = pipeline({'audio':wav})\n",
        "      tmp_segs = [((s.start,s.end),l) for s,_,l in cls.itertracks(yield_label=True)]\n",
        "      segs = []\n",
        "      prev_ind = None\n",
        "      prev_start = None\n",
        "      for (start,end),ind in tmp_segs+[((get_audio_duration({'audio':wav}),None),None)]:\n",
        "        if ind!=prev_ind:\n",
        "          if prev_ind is not None:\n",
        "            segs.append([(prev_start,start),prev_ind])\n",
        "          prev_ind = ind\n",
        "          prev_start = start\n",
        "    elif model=='pyAudioAnalysis':\n",
        "      !pip install hmmlearn==0.2.8\n",
        "      !pip install eyeD3==0.9.5\n",
        "      !pip install pydub==0.24.0\n",
        "      !pip install pyAudioAnalysis\n",
        "      from pyAudioAnalysis import audioSegmentation as aS\n",
        "      mid_window=2\n",
        "      mid_step=0.2\n",
        "      short_window=0.05\n",
        "      lda_dim=0 #35\n",
        "      cls = aS.speaker_diarization(wav, len(outputs), mid_window=mid_window, mid_step=mid_step, short_window=short_window, lda_dim=lda_dim)\n",
        "      segs = list(zip(*aS.labels_to_segments(cls, mid_step)))\n",
        "    deleted = 0\n",
        "    unified = 0\n",
        "    if min_dt:\n",
        "      for i in range(len(segs)-1,0,-1):\n",
        "        if segs[i][0][1]-segs[i][0][0]<min_dt:\n",
        "          if i+1<len(segs) and segs[i-1][1] == segs[i+1][1]:\n",
        "            segs[i-1] = ((segs[i-1][0][0],segs[i+1][0][1]),segs[i-1][1])\n",
        "            del segs[i+1]\n",
        "            unified += 1\n",
        "          else:\n",
        "            segs[i-1] = ((segs[i-1][0][0],segs[i][0][1]),segs[i-1][1])\n",
        "          del segs[i]\n",
        "          deleted += 1\n",
        "    inds = {}\n",
        "    my_ind = 0\n",
        "    with open('/content/list.txt','w',encoding='utf8') as f:\n",
        "      for i,((start,end),ind) in enumerate(segs):\n",
        "        if ind not in inds:\n",
        "          inds[ind] = my_ind%len(outputs)\n",
        "          my_ind += 1\n",
        "        f.write(\"file '%s'\\n\"%outputs[inds[ind]])\n",
        "        if i>0:\n",
        "          f.write('inpoint %f\\n'%start)\n",
        "        if i<len(segs)-1:\n",
        "          f.write('outpoint %f\\n'%end)\n",
        "    !ffmpeg -f concat -safe 0 -i /content/list.txt -i \"{outputs[0]}\" -map 0:v -map 1:a \\ -c:v libx264 -crf 18 -preset slow -c:a aac -b:a 192k \\ -vf \"crop=trunc(iw/2)*2:trunc(ih/2)*2,eq=contrast=1.1:brightness=0.05,unsharp=lx=3:ly=3:la=1.5:luma_gain=1.0, hqdn3d=1.5:1.5:6:6\" -pix_fmt yuv420p -profile:v high -movflags +faststart \\ /content/combined.mp4 -y\n",
        "    new_outputs = ['/content/combined.mp4']\n",
        "    if len(outputs)==2:\n",
        "      with open('/content/list2.txt','w',encoding='utf8') as f:\n",
        "        for i,((start,end),ind) in enumerate(segs):\n",
        "          f.write(\"file '%s'\\n\"%outputs[1-inds[ind]])\n",
        "          if i>0:\n",
        "            f.write('inpoint %f\\n'%start)\n",
        "          if i<len(segs)-1:\n",
        "            f.write('outpoint %f\\n'%end)\n",
        "      !ffmpeg -f concat -safe 0 -i /content/list.txt -i \"{outputs[0]}\" -map 0:v -map 1:a \\ -c:v libx264 -crf 18 -preset slow -c:a aac -b:a 192k \\ -vf \"crop=trunc(iw/2)*2:trunc(ih/2)*2,eq=contrast=1.1:brightness=0.05,unsharp=lx=3:ly=3:la=1.5:luma_gain=1.0, hqdn3d=1.5:1.5:6:6\" -pix_fmt yuv420p -profile:v high -movflags +faststart \\ /content/combined.mp4 -y\n",
        "      new_outputs.append('/content/combined2.mp4')\n",
        "    outputs = new_outputs\n",
        "\n",
        "  from IPython.display import HTML, clear_output\n",
        "  from base64 import b64encode\n",
        "\n",
        "  clear_output()\n",
        "  print('if video does not show below, you can still download it!')\n",
        "  if wav:\n",
        "    print('speakers=%d segments=%d deleted=%d unified=%d'%(len(inds), len(segs),deleted,unified))\n",
        "  muted = 'muted'\n",
        "\n",
        "  for i,file in enumerate(reversed(outputs)):\n",
        "    if i==len(outputs)-1:\n",
        "      muted = ''\n",
        "    try:\n",
        "      with open(file, 'rb') as f:\n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "      display(HTML(\"\"\"\n",
        "      <video width=600 controls autoplay loop %s>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "      </video>\"\"\" % (muted,data_url)))\n",
        "    except Exception:\n",
        "      pass\n",
        "  if wav:\n",
        "    print('speakers=%d segments=%d deleted=%d unified=%d'%(len(inds), len(segs),deleted,unified))\n",
        "  for file in outputs:\n",
        "    try:\n",
        "      files.download(file)\n",
        "    except Exception:\n",
        "      pass"
      ],
      "metadata": {
        "id": "Yixy6ek9IUlw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}